{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"weave \ud83d\udd2c A comprehensive snakemake pipeline for illumnia demultiplexing and fastq quality control and assurance <p>     This is the home of the pipeline, weave. Its long-term goals: provide a stable and user friendly interface to demultiplexing illumnia BCL files, exporting those deconvolved BCLs to FASTQ format, and checking the quality of resulting FASTQs (via multiple extendable methods).   </p>"},{"location":"#overview","title":"Overview","text":"<p>Welcome to weave's documentation! This guide is the main source of documentation for users that are getting started with the weave. </p> <p>The <code>./weave</code> pipeline is composed of two sub commands to setup and run the pipeline across different systems. Each of the available sub commands perform different functions: </p> <p><code>weave run</code>  Run the weave pipeline with your input files.</p> <p><code>weave cache</code> Downloads the reference files for the pipeline to a selected directory.</p> <p>weave is a two-pronged pipeline; the first prong detects and uses the appropriate illumnia software to demultiplex the ensemble collection of reads into their individual samples and converts the sequencing information into the FASTQ file format. From there out the second prong is a distrubted parallele step that uses a variety of commonly accepting nextgen sequencing tools to report, visualize, and calculate the quality of the reads after sequencing. weave makes uses of the ubiquitous containerization software singularity<sup>2</sup> for modularity, and the robust pipelining DSL Snakemake<sup>3</sup></p> <p>weave common use is to gauge the qualtiy of reads for potential downstream analysis. Since bioinformatic analysis requires robust and accurate data to draw scientific conclusions, this helps save time and resources when it comes to analyzing the volumous amount of sequencing data that is collected routinely.</p> <p>Several of the applications that weave uses to visualize and report quality metrics are: - Kraken<sup>7</sup>, kmer analysis - Kaiju<sup>4</sup>, kmer analysis - FastQC, fastq statistics - fastp<sup>6</sup>, fastq adapter removal (trimming) - FastQ Screen<sup>5</sup>, taxonomic quantification - MultiQC<sup>1</sup>, ensemble QC results</p>"},{"location":"#contribute","title":"Contribute","text":"<p>This site is a living document, created for and by members like you. weave is maintained by the members of NCBR and is improved by continous feedback! We encourage you to contribute new content and make improvements to existing content via pull request to our GitHub repository .</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use this software, please cite it as below:  </p> BibTexAPA <pre><code>Citation coming soon!\n</code></pre> <pre><code>Citation coming soon!\n</code></pre>"},{"location":"#references","title":"References","text":"<p><sup>1. Philip Ewels, M\u00e5ns Magnusson, Sverker Lundin, Max K\u00e4ller, MultiQC: summarize analysis results for multiple tools and samples in a single report, Bioinformatics, Volume 32, Issue 19, October 2016, Pages 3047\u20133048.</sup> <sup>2. Kurtzer GM, Sochat V, Bauer MW (2017). Singularity: Scientific containers for mobility of compute. PLoS ONE 12(5): e0177459.</sup> <sup>3. Koster, J. and S. Rahmann (2018). \"Snakemake-a scalable bioinformatics workflow engine.\" Bioinformatics 34(20): 3600.</sup> <sup>4. Menzel P., Ng K.L., Krogh A. (2016) Fast and sensitive taxonomic classification for metagenomics with Kaiju. Nat. Commun. 7:11257</sup> <sup>5. Wingett SW and Andrews S. FastQ Screen: A tool for multi-genome mapping and quality control [version 2; referees: 4 approved]. F1000Research 2018, 7:1338</sup> <sup>6. Shifu Chen, Yanqing Zhou, Yaru Chen, Jia Gu; fastp: an ultra-fast all-in-one FASTQ preprocessor, Bioinformatics, Volume 34, Issue 17, 1 September 2018, Pages i884\u2013i890.</sup> <sup>7. Wood, D.E., Lu, J. &amp; Langmead, B. Improved metagenomic analysis with Kraken 2. Genome Biol 20, 257 (2019).</sup> </p>"},{"location":"execution/","title":"Execution context","text":"<p>weave is capable of automatically distributing its pipeline jobs across a slurm cluster. The context for it's initial execution can be varied as well.</p> <p>The context is also centrally related to the configuration and setup of a particular cluster. Right now weave is configured to work with NIH clusters skyline, biowulf, and bigsky.</p> <p>Typical contexts of execution include:</p>"},{"location":"execution/#srun-real-time-execution-non-interactive","title":"srun (real time execution) (non-interactive)","text":"<p>The weave pipeline can be triggered from a head node in a non-interactive fashion:</p>"},{"location":"execution/#bigskyskyline","title":"Bigsky/Skyline","text":"<p>Note</p> <p>Dependency files for skyline and bigsky are now the same  Bigsky: <code>/data/openomics/bin/dependencies.sh</code> Skyline: <code>/data/openomics/bin/dependencies.sh</code></p> <pre><code>source ${dependencies}\nsrun --export=ALL \"weave run [keyword args] ${run_id}\"\n</code></pre> <p>Note</p> <p>srun by default exports all environmental variables from the executing environment and <code>--export=ALL</code> can be left off</p>"},{"location":"execution/#biowulf","title":"Biowulf","text":"<pre><code>srun --export=ALL \"module load snakemake singularity; weave run [keyword args] ${run_id}\"\n</code></pre>"},{"location":"execution/#srun-real-time-execution-interactive","title":"srun (real time execution) (interactive)","text":""},{"location":"execution/#bigskyskyline_1","title":"Bigsky/Skyline","text":"<p>Note</p> <p>Dependency files for skyline and bigsky are now the same  Bigsky: <code>/data/openomics/bin/dependencies.sh</code> Skyline: <code>/data/openomics/bin/dependencies.sh</code></p> <pre><code>&gt; # &lt;head node&gt;\nsrun --pty bash\n&gt; # &lt;compute node&gt;\nsource ${dependencies}\nweave run [keyword args] ${run_id}\n</code></pre>"},{"location":"execution/#biowulf_1","title":"Biowulf","text":"<pre><code>&gt; # &lt;head node&gt;\nsinteractive\n&gt; # &lt;compute node&gt;\nmodule purge\nmodule load snakemake singularity\nweave run [keyword args] ${run_id}\n</code></pre> <p>Biowulf uses environmental modules to control software. After executing the above you should see a message similar to:</p> <p>[+] Loading snakemake  7.XX.X on cnXXXX [+] Loading singularity  4.X.X  on cnXXXX</p>"},{"location":"execution/#sbatch-later-time-execution","title":"sbatch (later time execution)","text":""},{"location":"execution/#bigskyskyline_2","title":"Bigsky/Skyline","text":""},{"location":"execution/#sbatch-tempalte","title":"sbatch tempalte","text":"bigsky-skyline sbatch template<pre><code>#!/bin/bash\n#SBATCH --job-name=&lt;job_name&gt;\n#SBATCH --export=ALL\n#SBATCH --time=01-00:00:00\n#SBATCH --cpus-per-task=1\n#SBATCH --ntasks=1\n#SBATCH --mem=8g\n#SBATCH --output=&lt;stdout_file&gt;_%j.out\nsource ${dependencies}\nweave run \\\n-s /sequencing/root/dir \\\n-o output_dir \\\n&lt;run_id&gt;\n</code></pre> <p>This above script can serve as a template to create an sbatch script for weave. Update the psuedo-variables in the script to suit your particular needs then execute using sbatch command:</p> <pre><code>sbatch weave_script.sbatch\n</code></pre>"},{"location":"execution/#biowulf_2","title":"Biowulf","text":""},{"location":"execution/#sbatch-tempalte_1","title":"sbatch tempalte","text":"biowulf sbatch template<pre><code>#!/bin/bash\n#SBATCH --job-name=&lt;job_name&gt;\n#SBATCH --export=ALL\n#SBATCH --time=01-00:00:00\n#SBATCH --cpus-per-task=1\n#SBATCH --ntasks=1\n#SBATCH --mem=8g\n#SBATCH --output=&lt;stdout_file&gt;_%j.out\nmodule purge\nmodule load snakemake singularity\nweave run \\\n-s /sequencing/root/dir \\\n-o output_dir \\\n&lt;run_id&gt;\n</code></pre> <p>Same sbatch execution as bigsky/skyline.</p> <pre><code>sbatch weave_script.sbatch\n</code></pre>"},{"location":"install/","title":"Installation","text":"<p>weave is a stand alone python script that triggers execution of containerized subjobs defined in the workflow. weave does require a few dependencies currently, with plans to dwindle these requirements in the future.</p>"},{"location":"install/#setup","title":"Setup","text":"<p>Warning</p> <p>All requirements should be installed to a python virtual environment and not to the system python</p> <pre><code># clone repo\ngit clone https://github.com/OpenOmics/weave.git\ncd weave\n# create virtual environment\npython -m venv ~/.my_venv\n# activate environment\nsource ~/.my_venv/bin/activate\npip install -r requirements.txt \n</code></pre>"},{"location":"install/#snakemake-singularity","title":"Snakemake &amp; singularity","text":"<p>Note</p> <p>Additional requirements beyond those listed in <code>requirements.txt</code> are the <code>Snakemake</code> python package and the <code>singularity</code> containerization software</p>"},{"location":"install/#on-nih-servers","title":"On NIH servers","text":"Biowulf server<pre><code>module purge\nmodule load snakemake singularity`\n</code></pre> <p>Biowulf uses environmental modules to control software. After executing the above you should see a message similar to:</p> <p>[+] Loading snakemake  7.XX.X on cnXXXX [+] Loading singularity  4.X.X  on cnXXXX</p> Bigsky<pre><code>source /data/openomics/bin/dependencies.sh`\n</code></pre> <p>Bigsky uses spack to load modules so a consolidated conda environment with snakemake is activated:</p> dependencies.sh<pre><code>if [ ! -x \"$(command -v \"snakemake\")\" ]; then\n    source /gs1/apps/user/rmlspack/share/spack/setup-env.sh\n    export PS1=\"${PS1:-}\"\n    spack load -r miniconda3@4.11.0/y4vyh4u\n    source activate snakemake7-19-1\nfi\n# Add this folder to $PATH\nexport PATH=\"/data/openomics/bin:${PATH}\"\n# Add different pipelines to $PATH\nexport PATH=\"/data/openomics/prod/rna-seek/latest:${PATH}\"\nexport PATH=\"/data/openomics/prod/metavirs/latest:${PATH}\"\n</code></pre> <p>While, singularity is installed to the BigSky system and available upon login.</p>"},{"location":"install/#outside-nih-servers","title":"Outside NIH servers","text":"<p>Please follow the relevent instructions on the related package(s): Snakemake and singularity</p>"},{"location":"install/#quickstart","title":"Quickstart","text":"<p>After installing all the dependencies you then test the workflow functionality using a the dry run switch in the weave frontend.</p> <pre><code># after clone, dependencies, and installation\ncd weave # git repository root\n./weave run \\\n-s .tests/illumnia_demux \\ \n-o .tests/illumnia_demux/dry_run_out \\\n--local --dry-run /opt2/.tests/illumnia_demux\n</code></pre>"},{"location":"license/","title":"License","text":""},{"location":"license/#mit-license","title":"MIT License","text":"<p>Copyright \u00a9 2023 OpenOmics</p> <p><sub>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</sub></p> <p><sub>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</sub></p> <p><sub>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</sub></p>"},{"location":"ref/reference/","title":"Reference","text":"<p>Functions aiding and orchestrating the execution and validation of the weave workflow.</p>"},{"location":"ref/reference/#file-helper-functions","title":"File helper functions","text":""},{"location":"ref/reference/#scripts.files.get_all_seq_dirs","title":"<code>get_all_seq_dirs(top_dir, server)</code>","text":"<p>Gather and return all sequencing directories from the <code>top_dir</code>.  This is tightly coupled at the moment to the directory that is on RML-BigSky. In the future will need to the take a look at how to do this more generally</p> Source code in <code>scripts/files.py</code> <pre><code>def get_all_seq_dirs(top_dir, server):\n    \"\"\"\n        Gather and return all sequencing directories from the `top_dir`. \n        This is tightly coupled at the moment to the directory that is on RML-BigSky.\n        In the future will need to the take a look at how to do this more generally\n    \"\"\"\n    if isinstance(top_dir, str): top_dir = Path(top_dir)\n    _dirs = []\n    for _file in top_dir.glob('*'):\n        if _file.is_dir():\n            for _file2 in _file.glob('*'):\n                if _file2.is_dir() and check_access(_file2, R_OK):\n                    _dirs.append(_file2.resolve())\n    # check if directory is processed or not\n    return _dirs\n</code></pre>"},{"location":"ref/reference/#scripts.files.is_dir_staged","title":"<code>is_dir_staged(server, run_dir)</code>","text":"<p>filter check for wheter or not a directory has the appropriate breadcrumbs or not</p> <p>CopyComplete.txt - file transfer from instrument breadcrumb, blank (won't be there on instruments != NextSeq2k)</p> <p>RTAComplete.txt - sequencing breadcrumb, CSV file with values:     Run Date, Run time, Instrument ID</p> <p>RunInfo.xml - XML metainformation (RunID, Tiles, etc)</p> Source code in <code>scripts/files.py</code> <pre><code>def is_dir_staged(server, run_dir):\n    \"\"\"\n        filter check for wheter or not a directory has the appropriate breadcrumbs or not\n\n        CopyComplete.txt - file transfer from instrument breadcrumb, blank (won't be there on instruments != NextSeq2k)\n\n        RTAComplete.txt - sequencing breadcrumb, CSV file with values:\n            Run Date, Run time, Instrument ID\n\n        RunInfo.xml - XML metainformation (RunID, Tiles, etc)\n    \"\"\"\n    analyzed_checks = [\n        Path(run_dir, 'RTAComplete.txt').exists(),\n        Path(run_dir, 'SampleSheet.csv').exists(),\n        Path(run_dir, 'RunInfo.xml').exists(),\n    ]\n    return all(analyzed_checks)\n</code></pre>"},{"location":"ref/reference/#scripts.files.parse_samplesheet","title":"<code>parse_samplesheet(ss)</code>","text":"<p>Parse the sample sheet into data structure</p> Source code in <code>scripts/files.py</code> <pre><code>def parse_samplesheet(ss):\n    \"\"\"\n        Parse the sample sheet into data structure\n    \"\"\"\n    parser = sniff_samplesheet(ss)\n    return parser(ss)\n</code></pre>"},{"location":"ref/reference/#scripts.files.runid2samplesheet","title":"<code>runid2samplesheet(runid, top_dir=DIRECTORY_CONFIGS['bigsky']['seq'])</code>","text":"<p>Given a valid run id return the path to the sample sheet</p> Source code in <code>scripts/files.py</code> <pre><code>def runid2samplesheet(runid, top_dir=DIRECTORY_CONFIGS['bigsky']['seq']):\n    \"\"\"\n        Given a valid run id return the path to the sample sheet\n    \"\"\"\n    ss_path = Path(top_dir, runid)\n    if not ss_path.exists():\n        raise FileNotFoundError(f\"Run directory does not exist: {ss_path}\")\n    if Path(ss_path, f\"SampleSheet.txt\").exists():\n        ss_path = Path(ss_path, f\"SampleSheet.txt\")\n    elif Path(ss_path, f\"SampleSheet.csv\").exists():\n        ss_path = Path(ss_path, f\"SampleSheet.csv\")\n    elif Path(ss_path, f\"SampleSheet_{runid}.txt\").exists():\n        ss_path = Path(ss_path, f\"SampleSheet_{runid}.txt\")\n    elif Path(ss_path, f\"SampleSheet_{runid}.csv\").exists():\n        ss_path = Path(ss_path, f\"SampleSheet_{runid}.csv\")\n    else:\n        raise FileNotFoundError(\"Run sample sheet does not exist: \" + str(ss_path) + f\"/SampleSheet_{runid}.[txt, csv]\")\n    return ss_path\n</code></pre>"},{"location":"ref/reference/#scripts.files.sniff_samplesheet","title":"<code>sniff_samplesheet(ss)</code>","text":"<p>Given a sample sheet file return the appropriate function to parse the sheet.</p> Source code in <code>scripts/files.py</code> <pre><code>def sniff_samplesheet(ss):\n    \"\"\"\n        Given a sample sheet file return the appropriate function to parse the\n        sheet.\n    \"\"\"\n    return IllumniaSampleSheet\n</code></pre>"},{"location":"ref/reference/#utility-helper-functions","title":"Utility helper functions","text":""},{"location":"ref/reference/#scripts.utils.exec_pipeline","title":"<code>exec_pipeline(configs, dry_run=False, local=False)</code>","text":"<p>Execute the BCL-&gt;FASTQ pipeline.</p> <p>This executes the pipeline.</p> Source code in <code>scripts/utils.py</code> <pre><code>def exec_pipeline(configs, dry_run=False, local=False):\n    \"\"\"\n        Execute the BCL-&gt;FASTQ pipeline.\n\n        This executes the pipeline.\n    \"\"\"\n    this_instrument = 'Illumnia'\n    snake_file = SNAKEFILE[this_instrument]['ngs_qc']\n    fastq_demux_profile = DIRECTORY_CONFIGS[get_current_server()]['profile']\n    profile_config = {}\n    if Path(fastq_demux_profile, 'config.yaml').exists():\n        profile_config.update(yaml.safe_load(open(Path(fastq_demux_profile, 'config.yaml'))))\n\n    top_singularity_dirs = [Path(c_dir, '.singularity').absolute() for c_dir in configs['out_to']]\n    top_config_dirs = [Path(c_dir, '.config').absolute() for c_dir in configs['out_to']]\n    _dirs = top_singularity_dirs + top_config_dirs\n    mk_or_pass_dirs(*_dirs)\n    skip_config_keys = ('resources', 'runqc', 'use_scratch')\n\n    for i in range(0, len(configs['run_ids'])):\n        this_config = {k: (v[i] if k not in skip_config_keys else v) for k, v in configs.items() if v}\n        this_config.update(profile_config)\n\n        extra_to_mount = [this_config['out_to'], this_config['demux_input_dir']]\n        if this_config['bclconvert']:\n            bclcon_log_dir = Path(this_config['out_to'], \"logs\", \"bclconvert_demux\")\n            if not bclcon_log_dir.exists():\n                bclcon_log_dir.mkdir(mode=0o755, parents=True)\n            extra_to_mount.append(str(bclcon_log_dir) + \":\" + \"/var/log/bcl-convert:rw\")\n        if this_config.get('disambiguate', False):\n            extra_to_mount.append(Path(this_config['host_genome']).parent)\n            extra_to_mount.append(Path(this_config['pathogen_genome']).parent)\n        singularity_binds = get_mounts(*extra_to_mount)\n        config_file = Path(this_config['out_to'], '.config', f'config_job_{str(i)}.json').absolute()\n        json.dump(this_config, open(config_file, 'w'), cls=PathJSONEncoder, indent=4)\n        top_env = {}\n        top_env['PATH'] = os.environ[\"PATH\"]\n        top_env['SNK_CONFIG'] = str(config_file.absolute())\n        top_env['SINGULARITY_CACHEDIR'] = str(Path(this_config['out_to'], '.singularity').absolute())\n        this_cmd = [\n            \"snakemake\", \"-p\", \"--use-singularity\", \"--rerun-incomplete\", \"--keep-incomplete\",\n            \"--rerun-triggers\", \"mtime\", \"--verbose\", \"-s\", snake_file,\n        ]\n\n        if singularity_binds and not dry_run:\n            this_cmd.extend([\"--singularity-args\", f\"\\\"--env 'TMPDIR=/tmp' -C -B '{singularity_binds}'\\\"\"])\n\n        if dry_run:\n            print(f\"{esc_colors.OKGREEN}&gt; {esc_colors.ENDC}{esc_colors.UNDERLINE}Dry run{esc_colors.ENDC} \" + \\\n                  f\"demultiplexing of run {esc_colors.BOLD}{esc_colors.OKGREEN}{this_config['run_ids']}{esc_colors.ENDC}...\")\n            this_cmd.extend(['--dry-run'])\n        else:\n            if not local:\n                this_cmd.extend([\"--profile\", fastq_demux_profile])\n            print(f\"{esc_colors.OKGREEN}&gt; {esc_colors.ENDC}Executing ngs qc pipeline for run {esc_colors.BOLD}\"\n                  f\"{esc_colors.OKGREEN}{this_config['run_ids']}{esc_colors.ENDC}...\")\n\n        print(' '.join(map(str, this_cmd)))\n        exec_snakemake(this_cmd, local=local, dry_run=dry_run, env=top_env, cwd=str(Path(this_config['out_to']).absolute()))\n</code></pre>"},{"location":"ref/reference/#scripts.utils.valid_runid","title":"<code>valid_runid(id_to_check)</code>","text":"<p>Given an input ID get it's validity against the run id format:     YYMMDD_INSTRUMENTID_TIME_FLOWCELLID</p> Source code in <code>scripts/utils.py</code> <pre><code>def valid_runid(id_to_check):\n    '''\n        Given an input ID get it's validity against the run id format:\n            YYMMDD_INSTRUMENTID_TIME_FLOWCELLID\n    '''\n    id_to_check = str(id_to_check)\n    id_parts = id_to_check.split('_')\n    if len(id_parts) != 4:\n        raise ValueError(f\"Invalid run id format: {id_to_check}\")\n    try:\n        # YY MM DD\n        date_parser(id_parts[0])\n    except Exception as e:\n        raise ValueError('Invalid run id date') from e\n    try:\n        # HH MM\n        h = int(id_parts[2][0:3])\n        m = int(id_parts[2][2:])\n    except ValueError as e:\n        raise ValueError('Invalid run id time') from e\n\n    if h &gt;= 25 or m &gt;= 60:\n        raise ValueError('Invalid run id time: ' + h + m)\n\n    # TODO: check instruments against labkey\n    return id_to_check\n</code></pre>"},{"location":"ref/reference/#server-configuration-functions","title":"Server configuration functions","text":""},{"location":"ref/reference/#scripts.config.get_bigsky_seq_dirs","title":"<code>get_bigsky_seq_dirs()</code>","text":"<p>Get a list of sequence directories, that have the required illumnia file artifacts: RTAComplete.txt - breadcrumb file created by bigsky transfer process and illumnia sequencing</p> <p>Returns:</p> Type Description <code>list</code> <p>list of <code>pathlib.Path</code>s of all sequencing directories on bigsky server</p> Source code in <code>scripts/config.py</code> <pre><code>def get_bigsky_seq_dirs():\n    \"\"\"Get a list of sequence directories, that have the required illumnia file artifacts:\n    RTAComplete.txt - breadcrumb file created by bigsky transfer process and illumnia sequencing\n\n    Returns:\n        (list): list of `pathlib.Path`s of all sequencing directories on bigsky server\n    \"\"\"\n    top_dir = Path(\"/gs1/RTS/NextGen/SequencerRuns/\")\n    transfer_breadcrumb = \"RTAComplete.txt\"\n    if not top_dir.exists():\n        return None\n    seq_dirs = []\n    for this_dir in top_dir.iterdir():\n        if not this_dir.is_dir(): continue\n        for this_child_elem in this_dir.iterdir():\n            try:\n                elem_checks = [\n                    this_child_elem.is_dir(), \n                    Path(this_child_elem, transfer_breadcrumb).exists(),\n                    check_access(this_child_elem, R_OK)\n                ]\n            except (PermissionError, FileNotFoundError) as error:\n                continue\n            if all(elem_checks):\n                seq_dirs.append(this_child_elem.absolute())\n    return seq_dirs\n</code></pre>"},{"location":"ref/reference/#scripts.config.get_biowulf_seq_dirs","title":"<code>get_biowulf_seq_dirs()</code>","text":"<p>Get a list of sequence directories, that have the required illumnia file artifacts: RTAComplete.txt - breadcrumb file created by bigsky transfer process and illumnia sequencing</p> <p>Returns:</p> Type Description <code>list</code> <p>list of <code>pathlib.Path</code>s of all sequencing directories on biowulf server</p> Source code in <code>scripts/config.py</code> <pre><code>def get_biowulf_seq_dirs():\n    \"\"\"Get a list of sequence directories, that have the required illumnia file artifacts:\n    RTAComplete.txt - breadcrumb file created by bigsky transfer process and illumnia sequencing\n\n    Returns:\n        (list): list of `pathlib.Path`s of all sequencing directories on biowulf server\n    \"\"\"\n    top_dir = Path(\"/data/RTB_GRS/SequencerRuns/\")\n    transfer_breadcrumb = \"RTAComplete.txt\"\n    if not top_dir.exists():\n        return None\n    return [xx for x in top_dir.iterdir() if x.is_dir() for xx in x.iterdir() if xx.is_dir() and Path(xx, transfer_breadcrumb).exists()]\n</code></pre>"},{"location":"ref/reference/#scripts.config.get_current_server","title":"<code>get_current_server()</code>","text":"<p>Return the current server name by looking at the hostname</p> <p>Returns:</p> Type Description <code>str</code> <p>one of <code>bigsky</code>, <code>biowulf</code>, or <code>locus</code></p> Source code in <code>scripts/config.py</code> <pre><code>def get_current_server():\n    \"\"\"Return the current server name by looking at the hostname\n\n    Returns:\n        (str): one of `bigsky`, `biowulf`, or `locus`\n    \"\"\"\n    hn = gethostname()\n    # bigsky hostnames\n    re_bigsky = (r\"ai-rml.*\\.niaid\\.nih\\.gov\", \"bigsky\")\n\n    # biowulf hostnames\n    re_biowulf_head = (r\"biowulf\\.nih\\.gov\", \"biowulf\")\n    re_biowulf_compute = (r\"cn\\d{4}\", \"biowulf\")\n\n    # skyline hostnames\n    re_skyline_head = (r\"ai-hpc(submit|n)(\\d+)?\", \"skyline\")\n    re_skyline_compute = (r\"ai-hpc(submit|n)(\\d+)?\", \"skyline\")\n\n    host_profiles = [re_bigsky, re_biowulf_compute, re_biowulf_head, re_skyline_head, re_skyline_compute]\n\n    host = None\n    for pat, this_host in host_profiles:\n        if re.match(pat, hn):\n            host = this_host\n            break\n    if host is None:\n        raise ValueError(f\"Unknown host profile\")\n    return host\n</code></pre>"},{"location":"ref/reference/#scripts.config.get_resource_config","title":"<code>get_resource_config()</code>","text":"<p>Return a dictionary containing server specific references utilized in  the workflow for directories or reference files.</p> <p>Returns:</p> Type Description <code>dict</code> <p>return configuration key value pairs of current server::</p> <p>{ \"sif\": \"/server/location/to/sif/directory\", \"mounts\": {     \"refence binding\": {         \"to\": \"/bind/to\",         \"from\": \"/bind/from\",         \"mode\": \"ro/rw\"     },     ... }</p> Source code in <code>scripts/config.py</code> <pre><code>def get_resource_config():\n    \"\"\"Return a dictionary containing server specific references utilized in \n    the workflow for directories or reference files.\n\n    Returns:\n        (dict): return configuration key value pairs of current server::\n\n            {\n            \"sif\": \"/server/location/to/sif/directory\",\n            \"mounts\": {\n                \"refence binding\": {\n                    \"to\": \"/bind/to\",\n                    \"from\": \"/bind/from\",\n                    \"mode\": \"ro/rw\"\n                },\n                ...\n            }\n    \"\"\"\n    resource_dir = Path(__file__, '..', '..', 'config').absolute()\n    resource_json = Path(resource_dir, get_current_server() + '.json').resolve()\n\n    if not resource_json.exists():\n        return None\n\n    return json.load(open(resource_json))\n</code></pre>"},{"location":"usage/cache/","title":"weave cache","text":""},{"location":"usage/cache/#objective","title":"Objective","text":"<p>The weave pipeline utilizes a number of large file resources including containers, reference genomes, reference databases, and indexes. This command allows for the user to download all these files serially in one-shot. This execution method will updated to include a parallel cluster option for faster execution.</p>"},{"location":"usage/cache/#execution","title":"Execution","text":""},{"location":"usage/cache/#example-command","title":"Example command","text":"cache commmand<pre><code># starting from install\n./weave cache ./output_directory/\n</code></pre>"},{"location":"usage/cache/#output","title":"Output","text":"<p>Warning: cache download only implemented in serial local mode currently Getting docker resource bcl2fastq... ...singularity output... Getting docker resource weave... ...singularity output... Getting web resource kraken... ...progress bar... Getting web resource kaiju... ...progress bar... Cache downloads complete!</p>"},{"location":"usage/cache/#contents","title":"Contents","text":"<p>The current contents of what all is downloaded via the cache command are:</p> <ol> <li>All pipeline containerized images in read-only Singularity Image Format (SIF)</li> <li>Kraken2 kmer databases</li> <li>Kaiju kmer databases</li> <li>FastQ_Screen genome indexes</li> </ol>"},{"location":"usage/run/","title":"weave run","text":""},{"location":"usage/run/#objective","title":"Objective","text":"<p><code>weave</code>'s <code>run</code> subcommand is the command line entrypoint for execution of the Illumnia demultiplexing and FASTQ QC/QA pipeline. </p> <p>Given a run id or run directory containing the artifacts produced by Illumnia MiSeq/NextSeq/HiSeq instruments (RTAComplete.txt) this workflow will demultiplex all the reads and run an array of tools for evaluating the qualitying of the sequenced reads.</p> <p>The ultimate output for this workflow is a MultiQC report containing all the subsequent metrics from the QC/QA portions of the pipeline. </p> <p>The penultimate output for this workflow is a cohort of fastq.gz files deconvoluted from their adapter information and seperated into </p>"},{"location":"usage/run/#method-signature","title":"Method signature","text":"<pre><code>weave run [-h] \n    [-s/--seq_dir &lt;sequencing directory&gt;] \n    [-o/--output &lt;output directory&gt;] \n    [-d/--dry-run] \n    [-n/--noqc] \n    [-l/--local] \n    &lt;run directory&gt; [&lt;run directory&gt; ...]\n</code></pre>"},{"location":"usage/run/#example","title":"Example","text":""},{"location":"usage/run/#setup","title":"Setup","text":"<pre><code># Step 1.) Grab an interactive node,\n# do not run on head node!\nsinteractive\n\n# Step 2.) Follow install directions\nsource ~/.my_virtual_environment/bin/activate\n</code></pre>"},{"location":"usage/run/#dependencies","title":"Dependencies","text":"<p>Ensure that snakemake and singularity are loaded and <code>$PATH</code> accessible.</p> <p>If executing from biowulf cluster:</p> <pre><code># Step 3.) module load snakemake and depdenencies \nmodule purge\nmodule load singularity snakemake\n</code></pre> <p>If executing from BigSky cluster:</p> <pre><code># Step 4.) spack load snakemake and depdenencies\nsource /data/openomics/bin/dependencies.sh\n</code></pre>"},{"location":"usage/run/#execution","title":"Execution","text":"<pre><code># Step 4A.) Dry-run the weave pipeline\n./weave run &lt;run id&gt; \\\n                  --output /data/$USER/output \\\n                  --dry-run\n\n# Step 4B.) Run the weave pipeline\n# The slurm mode will submit jobs to \n# the cluster. It is recommended running \n# the pipeline in this mode.\n./weave run &lt;run id&gt; \\\n                  --output /data/$USER/output \\\n</code></pre>"},{"location":"usage/run/#run-identifiers-runid","title":"Run Identifiers (runid)","text":"<p>Run identifiers are strings that uniquely identify a particular sequencing run and are of the format:</p> <p><code>DATE&lt;YYMMDD&gt;_INSTRUMENTID_TIME&lt;24hr&gt;_FLOWCELLID</code></p>"},{"location":"usage/run/#arguments","title":"Arguments","text":""},{"location":"usage/run/#required","title":"Required","text":"<p>Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code.</p> <p><code>&lt;run directory&gt; [&lt;run directory&gt; ...]</code> </p> <p>Input runid or run directory. type: strings(s)/path(s) </p> <p>One or more IDs or directories can be provided and the pipeline will run them.  It is not necessary to specify server configuration information such as data parent directory, unless you on a non-standard cluster.</p> <p>Example: <code>220729_NB551182_0219_AHGGJNBGXK</code></p> <p><code>--output OUTPUT</code></p> <p>Path to the top-level output directory. type: strings(s)/path(s) </p> <p>This location is where the pipeline will create all of its output files. If the provided output directory does not exist, it will be created automatically.</p> <p>Example: <code>--output /data/$USER/weave_out</code></p>"},{"location":"usage/run/#optional","title":"Optional","text":"<p>Each of the following arguments are optional, and do not need to be provided. </p> <p><code>--dry-run</code> </p> <p>Dry run the pipeline. type: boolean flag</p> <p>Displays what steps in the pipeline remain or will be run. Does not execute anything!</p> <p>Example: <code>--dry-run</code></p> <p><code>--local</code> </p> <p>Execute locally instead of through a cluster executor type: boolean flag</p> <p>This flag will trigger the workflow to run in the local terminal as a blocking process.</p> <p>Example: <code>--local</code></p>"}]}